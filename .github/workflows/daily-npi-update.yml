# Daily NPI Update Workflow - Scrapes previous 3 days and updates rankings
# Save as: .github/workflows/daily-npi-update.yml

name: Daily NPI Rankings Update

# WHEN should this run?
on:
  schedule:
    # Run every day at 8:00 AM UTC (4 AM EST / 3 AM EST depending on season)
    - cron: '0 8 * * *'
  
  # Allow manual triggering from GitHub website
  workflow_dispatch:
    inputs:
      days_back:
        description: 'Number of days back to scrape (default: 3)'
        required: false
        default: '3'
        type: string

# WHAT should this do?
jobs:
  update-npi-rankings:
    runs-on: ubuntu-latest
    
    steps:
    # Step 1: Get your code from the repository
    - name: Checkout repository
      uses: actions/checkout@v4
      
    # Step 2: Set up Python
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    # Step 3: Install Chrome and ChromeDriver (for Selenium)
    - name: Install Chrome
      run: |
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
    # Step 4: Install Python dependencies
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium webdriver-manager pandas
        
    # Step 5: Run daily rolling scraper
    - name: Scrape recent games
      run: |
        echo "üîÑ Running daily rolling scraper..."
        python scripts/daily_rolling_scraper.py
        
    # Step 6: Calculate NPI rankings
    - name: Calculate NPI rankings
      run: |
        echo "üßÆ Calculating NPI rankings..."
        python scripts/daily_npi_calculator.py
        
    # Step 7: Check if we have new data to commit
    - name: Check for changes
      id: changes
      run: |
        if git diff --quiet; then
          echo "has_changes=false" >> $GITHUB_OUTPUT
          echo "No changes detected"
        else
          echo "has_changes=true" >> $GITHUB_OUTPUT
          echo "Changes detected"
          git diff --name-only
        fi
        
    # Step 8: Commit and push changes (only if there are changes)
    - name: Commit and push changes
      if: steps.changes.outputs.has_changes == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        git add data/
        git commit -m "ü§ñ Daily NPI update: $(date '+%Y-%m-%d %H:%M UTC')"
        git push
        
    # Step 9: Create a summary comment (optional)
    - name: Display summary
      run: |
        echo "üìä DAILY UPDATE SUMMARY"
        echo "======================"
        echo "Date: $(date)"
        
        if [ -f "data/npi_summary.json" ]; then
          echo "üìà Latest rankings summary:"
          python -c "
        import json
        try:
            with open('data/npi_summary.json', 'r') as f:
                summary = json.load(f)
            print(f'  Total teams ranked: {summary.get(\"total_teams\", 0)}')
            print(f'  Tournament eligible: {summary.get(\"tournament_eligible_teams\", 0)}')
            print(f'  Games processed: {summary.get(\"total_games\", 0)}')
            print(f'  Last updated: {summary.get(\"last_updated\", \"Unknown\")[:19]}')
            
            top_5 = summary.get('top_10', [])[:5]
            if top_5:
                print('\nüèÜ Current Top 5:')
                for i, team in enumerate(top_5, 1):
                    npi = team.get('npi', 0)
                    wins = team.get('wins', 0)
                    losses = team.get('losses', 0)
                    ties = team.get('ties', 0)
                    print(f'  {i}. {team.get(\"team\", \"Unknown\")} - {npi:.3f} ({wins}-{losses}-{ties})')
        except Exception as e:
            print(f'Error reading summary: {e}')
        "
        else
          echo "‚ùå No summary file found"
        fi
        
        if [ -f "data/scraping_log.json" ]; then
          echo -e "\nüìÖ Latest scraping activity:"
          python -c "
        import json
        try:
            with open('data/scraping_log.json', 'r') as f:
                log = json.load(f)
            if log:
                latest = log[-1]
                run_info = latest.get('run_info', {})
                print(f'  Dates attempted: {run_info.get(\"dates_attempted\", 0)}')
                print(f'  Successful: {run_info.get(\"dates_successful\", 0)}')
                print(f'  New games found: {run_info.get(\"total_new_games\", 0)}')
                print(f'  Total games in DB: {run_info.get(\"total_games_in_db\", 0)}')
        except Exception as e:
            print(f'Error reading log: {e}')
        "
        fi